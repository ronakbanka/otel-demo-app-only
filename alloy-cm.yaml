apiVersion: v1
data:
  config.alloy: |-
    discovery.kubernetes "nodes" {
      role = "node"
    }

    discovery.kubernetes "services" {
      role = "service"
    }

    discovery.kubernetes "endpoints" {
      role = "endpoints"
    }

    discovery.kubernetes "pods" {
      role = "pod"
    }

    // OTLP Receivers
    otelcol.receiver.otlp "receiver" {
      debug_metrics {
        disable_high_cardinality_metrics = true
      }

      grpc {
        endpoint = "0.0.0.0:4317"
      }

      http {
        endpoint = "0.0.0.0:4318"
      }
      output {
        metrics = [otelcol.processor.resourcedetection.default.input]
        logs = [otelcol.processor.resourcedetection.default.input]
        traces = [
          otelcol.processor.resourcedetection.default.input,
          otelcol.connector.spanmetrics.default.input,
          otelcol.connector.servicegraph.default.input,
          ]
      }
    }


    // Zipkin Receiver
    otelcol.receiver.zipkin "receiver" {
      debug_metrics {
        disable_high_cardinality_metrics = true
      }

    endpoint = "0.0.0.0:9411"
      output {
        traces = [otelcol.processor.k8sattributes.default.input]
      }
    }


    // Processors
    otelcol.processor.transform "add_metric_datapoint_attributes" {
      // Grafana Cloud Kubernetes monitoring expects Loki labels `cluster`, `pod`, and `namespace`
      error_mode = "ignore"
      metric_statements {
        context = "datapoint"
        statements = [
          "set(attributes[\"deployment.environment\"], resource.attributes[\"deployment.environment\"])",
          "set(attributes[\"service.version\"], resource.attributes[\"service.version\"])",
        ]
      }
      output {
        metrics = [otelcol.processor.k8sattributes.default.input]
      }
    }

    otelcol.processor.resourcedetection "default" {
      detectors = ["env", "system"]

      system {
        hostname_sources = ["os"]
      }

      output {
        metrics = [otelcol.processor.transform.add_metric_datapoint_attributes.input]
        logs    = [otelcol.processor.k8sattributes.default.input]
        traces  = [otelcol.processor.k8sattributes.default.input]
      }
    }

    otelcol.processor.k8sattributes "default" {
      extract {
        metadata = [
          "k8s.namespace.name",
          "k8s.pod.name",
          "k8s.deployment.name",
          "k8s.statefulset.name",
          "k8s.daemonset.name",
          "k8s.cronjob.name",
          "k8s.job.name",
          "k8s.node.name",
          "k8s.pod.uid",
          "k8s.pod.start_time",
        ]
      }
      pod_association {
        source {
          from = "connection"
        }
      }

      output {
        metrics = [otelcol.processor.transform.default.input]
        logs    = [otelcol.processor.transform.default.input]
        traces  = [
          otelcol.processor.transform.default.input,
          otelcol.connector.host_info.default.input,
        ]
      }
    }
    otelcol.connector.host_info "default" {
      host_identifiers = [ "k8s.node.name" ]
      output {
        metrics = [otelcol.processor.batch.host_info_batch.input]
      }
    }

    otelcol.processor.batch "host_info_batch" {
      output {
        metrics = [otelcol.exporter.prometheus.host_info_metrics.input]
      }
    }

    otelcol.exporter.prometheus "host_info_metrics" {
      add_metric_suffixes = false
      forward_to = [prometheus.remote_write.metrics_service.receiver]
    }

    // The Spanmetrics Connector will generate RED metrics based on the incoming trace span data.
    otelcol.connector.spanmetrics "default" {
        // The namespace explicit adds a prefix to all the generated span metrics names.
        // In this case, we'll ensure they match as closely as possible those generated by Tempo.

        // Each extra dimension (metrics label) to be added to the generated metrics from matching span attributes. These
        // need to be defined with a name and optionally a default value (in the following cases, we do not want a default
        // value if the span attribute is not present).
        dimension {
            name = "http.method"
        }
        dimension {
            name = "http.status_code"
        }
        metrics_flush_interval = "5s"

        // A histogram block must be present, either explicitly defining bucket values or via an exponential block.
        // We do the latter here.
        histogram {
            explicit {
              buckets=  ["10ms", "50ms", "100ms", "200ms", "400ms", "1s"]
            }
        }

        // The exemplar block is added to ensure we generate exemplars for traces on relevant metric values.
        exemplars {
            enabled = true
        }

        // Generated metrics data is in OTLP format. We send this data to the OpenTelemetry Prometheus exporter to ensure
        // it gets transformed into Prometheus format data.
        output {
            // metrics = [otelcol.exporter.prometheus.metrics_converter.input]
            metrics = [otelcol.processor.transform.add_metric_datapoint_attributes.input]
        }
    }

    otelcol.connector.servicegraph "default" {
       dimensions = ["http.method"]
       output {
          metrics = [otelcol.processor.transform.add_metric_datapoint_attributes.input]
       }
    }

    otelcol.processor.transform "default" {
      // Grafana Cloud Kubernetes monitoring expects Loki labels `cluster`, `pod`, and `namespace`
      error_mode = "ignore"
      metric_statements {
        context = "resource"
        statements = [
          "set(attributes[\"k8s.cluster.name\"], \"aks-cluster\") where attributes[\"k8s.cluster.name\"] == nil",
        ]
      }
      log_statements {
        context = "resource"
        statements = [
          "set(attributes[\"pod\"], attributes[\"k8s.pod.name\"])",
          "set(attributes[\"namespace\"], attributes[\"k8s.namespace.name\"])",
          "set(attributes[\"loki.resource.labels\"], \"pod, namespace, cluster, job\")",
          "set(attributes[\"k8s.cluster.name\"], \"aks-cluster\") where attributes[\"k8s.cluster.name\"] == nil",
        ]
      }
      trace_statements {
        context = "resource"
        statements = [
          "set(attributes[\"k8s.cluster.name\"], \"aks-cluster\") where attributes[\"k8s.cluster.name\"] == nil",
        ]
      }
      output {
        metrics = [otelcol.processor.filter.default.input]
        logs = [otelcol.processor.filter.default.input]
        traces = [otelcol.processor.filter.default.input]
      }
    }

    otelcol.processor.filter "default" {
      error_mode = "ignore"

      output {
        metrics = [otelcol.processor.batch.batch_processor.input]
        logs = [otelcol.processor.batch.batch_processor.input]
        traces = [otelcol.processor.batch.batch_processor.input]
      }
    }

    otelcol.processor.batch "batch_processor" {
      send_batch_size = 16384
      send_batch_max_size = 0
      timeout = "2s"
      output {
        metrics = [otelcol.exporter.prometheus.metrics_converter.input]
        logs = [otelcol.exporter.loki.logs_converter.input]
        traces = [otelcol.exporter.otlp.traces_service.input]
      }
    }
    otelcol.exporter.prometheus "metrics_converter" {
      forward_to = [prometheus.relabel.metrics_service.receiver]
    }
    otelcol.exporter.loki "logs_converter" {
      forward_to = [loki.process.pod_logs.receiver]
    }
    // Annotation Autodiscovery
    discovery.relabel "annotation_autodiscovery_pods" {
      targets = discovery.kubernetes.pods.targets
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_scrape"]
        regex = "true"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_job"]
        action = "replace"
        target_label = "job"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_instance"]
        action = "replace"
        target_label = "instance"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_path"]
        action = "replace"
        target_label = "__metrics_path__"
      }

      // Choose the pod port
      // The discovery generates a target for each declared container port of the pod.
      // If the metricsPortName annotation has value, keep only the target where the port name matches the one of the annotation.
      rule {
        source_labels = ["__meta_kubernetes_pod_container_port_name"]
        target_label = "__tmp_port"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_portName"]
        regex = "(.+)"
        target_label = "__tmp_port"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_container_port_name"]
        action = "keepequal"
        target_label = "__tmp_port"
      }

      // If the metrics port number annotation has a value, override the target address to use it, regardless whether it is
      // one of the declared ports on that Pod.
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_portNumber", "__meta_kubernetes_pod_ip"]
        regex = "(\\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})"
        replacement = "[$2]:$1" // IPv6
        target_label = "__address__"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_portNumber", "__meta_kubernetes_pod_ip"]
        regex = "(\\d+);((([0-9]+?)(\\.|$)){4})" // IPv4, takes priority over IPv6 when both exists
        replacement = "$2:$1"
        target_label = "__address__"
      }

      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_scheme"]
        action = "replace"
        target_label = "__scheme__"
      }
    }

    discovery.relabel "annotation_autodiscovery_services" {
      targets = discovery.kubernetes.services.targets
      rule {
        source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_scrape"]
        regex = "true"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_job"]
        action = "replace"
        target_label = "job"
      }
      rule {
        source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_instance"]
        action = "replace"
        target_label = "instance"
      }
      rule {
        source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_metrics_path"]
        action = "replace"
        target_label = "__metrics_path__"
      }

      // Choose the service port
      rule {
        source_labels = ["__meta_kubernetes_service_port_name"]
        target_label = "__tmp_port"
      }
      rule {
        source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_metrics_portName"]
        regex = "(.+)"
        target_label = "__tmp_port"
      }
      rule {
        source_labels = ["__meta_kubernetes_service_port_name"]
        action = "keepequal"
        target_label = "__tmp_port"
      }

      rule {
        source_labels = ["__meta_kubernetes_service_port_number"]
        target_label = "__tmp_port"
      }
      rule {
        source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_metrics_portNumber"]
        regex = "(.+)"
        target_label = "__tmp_port"
      }
      rule {
        source_labels = ["__meta_kubernetes_service_port_number"]
        action = "keepequal"
        target_label = "__tmp_port"
      }

      rule {
        source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_metrics_scheme"]
        action = "replace"
        target_label = "__scheme__"
      }
    }

    discovery.relabel "annotation_autodiscovery_http" {
      targets = concat(discovery.relabel.annotation_autodiscovery_pods.output, discovery.relabel.annotation_autodiscovery_services.output)
      rule {
        source_labels = ["__scheme__"]
        regex = "https"
        action = "drop"
      }
    }

    discovery.relabel "annotation_autodiscovery_https" {
      targets = concat(discovery.relabel.annotation_autodiscovery_pods.output, discovery.relabel.annotation_autodiscovery_services.output)
      rule {
        source_labels = ["__scheme__"]
        regex = "https"
        action = "keep"
      }
    }

    prometheus.scrape "annotation_autodiscovery_http" {
      targets = discovery.relabel.annotation_autodiscovery_http.output
      honor_labels = true
      clustering {
        enabled = true
      }
      forward_to = [prometheus.relabel.annotation_autodiscovery.receiver]
    }

    prometheus.scrape "annotation_autodiscovery_https" {
      targets = discovery.relabel.annotation_autodiscovery_https.output
      honor_labels = true
      bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
      tls_config {
        insecure_skip_verify = true
      }
      clustering {
        enabled = true
      }
      forward_to = [prometheus.relabel.annotation_autodiscovery.receiver]
    }

    prometheus.relabel "annotation_autodiscovery" {
      max_cache_size = 100000
      forward_to = [prometheus.relabel.metrics_service.receiver]
    }

    // Grafana Alloy
    discovery.relabel "alloy" {
      targets = discovery.kubernetes.pods.targets
      rule {
        source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_instance"]
        regex = "grafana-k8s-monitoring"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_name"]
        regex = "alloy.*"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_container_port_name"]
        regex = "http-metrics"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_namespace"]
        target_label  = "namespace"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_name"]
        target_label  = "pod"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_container_name"]
        target_label  = "container"
      }
    }

    prometheus.scrape "alloy" {
      job_name = "integrations/alloy"
      targets = discovery.relabel.alloy.output
      scrape_interval = "60s"
      forward_to = [prometheus.relabel.alloy.receiver]
      clustering {
        enabled = true
      }
    }

    prometheus.relabel "alloy" {
      max_cache_size = 100000
      rule {
        source_labels = ["__name__"]
        regex = "up|alloy_build_info"
        action = "keep"
      }
      forward_to = [prometheus.relabel.metrics_service.receiver]
    }

    // Kubernetes Monitoring Telemetry
    prometheus.exporter.unix "kubernetes_monitoring_telemetry" {
      set_collectors = ["textfile"]
      textfile {
        directory = "/etc/kubernetes-monitoring-telemetry"
      }
    }

    prometheus.scrape "kubernetes_monitoring_telemetry" {
      job_name   = "integrations/kubernetes/kubernetes_monitoring_telemetry"
      targets    = prometheus.exporter.unix.kubernetes_monitoring_telemetry.targets
      scrape_interval = "60s"
      clustering {
        enabled = true
      }
      forward_to = [prometheus.relabel.kubernetes_monitoring_telemetry.receiver]
    }

    prometheus.relabel "kubernetes_monitoring_telemetry" {
      max_cache_size = 100000
      rule {
        target_label = "job"
        action = "replace"
        replacement = "integrations/kubernetes/kubernetes_monitoring_telemetry"
      }
      rule {
        target_label = "instance"
        action = "replace"
        replacement = "grafana-k8s-monitoring"
      }
      rule {
        source_labels = ["__name__"]
        regex = "up|grafana_kubernetes_monitoring_.*"
        action = "keep"
      }
      forward_to = [prometheus.relabel.metrics_service.receiver]
    }

    // Kubelet
    discovery.relabel "kubelet" {
      targets = discovery.kubernetes.nodes.targets
    }

    prometheus.scrape "kubelet" {
      job_name   = "integrations/kubernetes/kubelet"
      targets  = discovery.relabel.kubelet.output
      scheme   = "https"
      scrape_interval = "60s"
      bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
      tls_config {
        insecure_skip_verify = true
      }
      clustering {
        enabled = true
      }
      forward_to = [prometheus.relabel.kubelet.receiver]
    }

    prometheus.relabel "kubelet" {
      max_cache_size = 100000
      rule {
        source_labels = ["__name__"]
        regex = "up|container_cpu_usage_seconds_total|kubelet_certificate_manager_client_expiration_renew_errors|kubelet_certificate_manager_client_ttl_seconds|kubelet_certificate_manager_server_ttl_seconds|kubelet_cgroup_manager_duration_seconds_bucket|kubelet_cgroup_manager_duration_seconds_count|kubelet_node_config_error|kubelet_node_name|kubelet_pleg_relist_duration_seconds_bucket|kubelet_pleg_relist_duration_seconds_count|kubelet_pleg_relist_interval_seconds_bucket|kubelet_pod_start_duration_seconds_bucket|kubelet_pod_start_duration_seconds_count|kubelet_pod_worker_duration_seconds_bucket|kubelet_pod_worker_duration_seconds_count|kubelet_running_container_count|kubelet_running_containers|kubelet_running_pod_count|kubelet_running_pods|kubelet_runtime_operations_errors_total|kubelet_runtime_operations_total|kubelet_server_expiration_renew_errors|kubelet_volume_stats_available_bytes|kubelet_volume_stats_capacity_bytes|kubelet_volume_stats_inodes|kubelet_volume_stats_inodes_used|kubernetes_build_info|namespace_workload_pod|rest_client_requests_total|storage_operation_duration_seconds_count|storage_operation_errors_total|volume_manager_total_volumes"
        action = "keep"
      }
      forward_to = [prometheus.relabel.metrics_service.receiver]
    }

    // cAdvisor
    discovery.relabel "cadvisor" {
      targets = discovery.kubernetes.nodes.targets
      rule {
        replacement   = "/metrics/cadvisor"
        target_label  = "__metrics_path__"
      }
    }

    prometheus.scrape "cadvisor" {
      job_name   = "integrations/kubernetes/cadvisor"
      targets    = discovery.relabel.cadvisor.output
      scheme     = "https"
      scrape_interval = "60s"
      bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
      tls_config {
        insecure_skip_verify = true
      }
      clustering {
        enabled = true
      }
      forward_to = [prometheus.relabel.cadvisor.receiver]
    }

    prometheus.relabel "cadvisor" {
      max_cache_size = 100000
      rule {
        source_labels = ["__name__"]
        regex = "up|container_cpu_cfs_periods_total|container_cpu_cfs_throttled_periods_total|container_cpu_usage_seconds_total|container_fs_reads_bytes_total|container_fs_reads_total|container_fs_writes_bytes_total|container_fs_writes_total|container_memory_cache|container_memory_rss|container_memory_swap|container_memory_working_set_bytes|container_network_receive_bytes_total|container_network_receive_packets_dropped_total|container_network_receive_packets_total|container_network_transmit_bytes_total|container_network_transmit_packets_dropped_total|container_network_transmit_packets_total|machine_memory_bytes"
        action = "keep"
      }
      // Drop empty container labels, addressing https://github.com/google/cadvisor/issues/2688
      rule {
        source_labels = ["__name__","container"]
        separator = "@"
        regex = "(container_cpu_.*|container_fs_.*|container_memory_.*)@"
        action = "drop"
      }
      // Drop empty image labels, addressing https://github.com/google/cadvisor/issues/2688
      rule {
        source_labels = ["__name__","image"]
        separator = "@"
        regex = "(container_cpu_.*|container_fs_.*|container_memory_.*|container_network_.*)@"
        action = "drop"
      }
      // Normalizing unimportant labels (not deleting to continue satisfying <label>!="" checks)
      rule {
        source_labels = ["__name__", "boot_id"]
        separator = "@"
        regex = "machine_memory_bytes@.*"
        target_label = "boot_id"
        replacement = "NA"
      }
      rule {
        source_labels = ["__name__", "system_uuid"]
        separator = "@"
        regex = "machine_memory_bytes@.*"
        target_label = "system_uuid"
        replacement = "NA"
      }
      // Filter out non-physical devices/interfaces
      rule {
        source_labels = ["__name__", "device"]
        separator = "@"
        regex = "container_fs_.*@(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dasd.+)"
        target_label = "__keepme"
        replacement = "1"
      }
      rule {
        source_labels = ["__name__", "__keepme"]
        separator = "@"
        regex = "container_fs_.*@"
        action = "drop"
      }
      rule {
        source_labels = ["__name__"]
        regex = "container_fs_.*"
        target_label = "__keepme"
        replacement = ""
      }
      rule {
        source_labels = ["__name__", "interface"]
        separator = "@"
        regex = "container_network_.*@(en[ospx][0-9].*|wlan[0-9].*|eth[0-9].*)"
        target_label = "__keepme"
        replacement = "1"
      }
      rule {
        source_labels = ["__name__", "__keepme"]
        separator = "@"
        regex = "container_network_.*@"
        action = "drop"
      }
      rule {
        source_labels = ["__name__"]
        regex = "container_network_.*"
        target_label = "__keepme"
        replacement = ""
      }
      forward_to = [prometheus.relabel.metrics_service.receiver]
    }

    // Kube State Metrics
    discovery.relabel "kube_state_metrics" {
      targets = discovery.kubernetes.services.targets
      rule {
        source_labels = ["__meta_kubernetes_service_label_app_kubernetes_io_instance"]
        regex = "grafana-k8s-monitoring"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_service_label_app_kubernetes_io_name"]
        regex = "kube-state-metrics"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_service_port_name"]
        regex = "http"
        action = "keep"
      }
    }

    prometheus.scrape "kube_state_metrics" {
      job_name   = "integrations/kubernetes/kube-state-metrics"
      targets    = discovery.relabel.kube_state_metrics.output
      scrape_interval = "60s"
      clustering {
        enabled = true
      }
      forward_to = [prometheus.relabel.kube_state_metrics.receiver]
    }

    prometheus.relabel "kube_state_metrics" {
      max_cache_size = 100000
      rule {
        source_labels = ["__name__"]
        regex = "up|kube_daemonset.*|kube_deployment_metadata_generation|kube_deployment_spec_replicas|kube_deployment_status_observed_generation|kube_deployment_status_replicas_available|kube_deployment_status_replicas_updated|kube_horizontalpodautoscaler_spec_max_replicas|kube_horizontalpodautoscaler_spec_min_replicas|kube_horizontalpodautoscaler_status_current_replicas|kube_horizontalpodautoscaler_status_desired_replicas|kube_job.*|kube_namespace_status_phase|kube_node.*|kube_persistentvolumeclaim_resource_requests_storage_bytes|kube_pod_container_info|kube_pod_container_resource_limits|kube_pod_container_resource_requests|kube_pod_container_status_last_terminated_reason|kube_pod_container_status_restarts_total|kube_pod_container_status_waiting_reason|kube_pod_info|kube_pod_owner|kube_pod_start_time|kube_pod_status_phase|kube_pod_status_reason|kube_replicaset.*|kube_resourcequota|kube_statefulset.*"
        action = "keep"
      }
      forward_to = [prometheus.relabel.metrics_service.receiver]
    }

    // Node Exporter
    discovery.relabel "node_exporter" {
      targets = discovery.kubernetes.pods.targets
      rule {
        source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_instance"]
        regex = "grafana-k8s-monitoring"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_name"]
        regex = "prometheus-node-exporter.*"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_node_name"]
        action = "replace"
        target_label = "instance"
      }
    }

    prometheus.scrape "node_exporter" {
      job_name   = "integrations/node_exporter"
      targets  = discovery.relabel.node_exporter.output
      scrape_interval = "60s"
      clustering {
        enabled = true
      }
      forward_to = [prometheus.relabel.node_exporter.receiver]
    }

    prometheus.relabel "node_exporter" {
      max_cache_size = 100000
      rule {
        source_labels = ["__name__"]
        regex = "up|node_cpu.*|node_exporter_build_info|node_filesystem.*|node_memory.*|process_cpu_seconds_total|process_resident_memory_bytes"
        action = "keep"
      }
      // Drop metrics for certain file systems
      rule {
        source_labels = ["__name__", "fstype"]
        separator = "@"
        regex = "node_filesystem.*@(tempfs)"
        action = "drop"
      }
      forward_to = [prometheus.relabel.metrics_service.receiver]
    }

    // OpenCost
    discovery.relabel "opencost" {
      targets = discovery.kubernetes.services.targets
      rule {
        source_labels = ["__meta_kubernetes_service_label_app_kubernetes_io_instance"]
        regex = "grafana-k8s-monitoring"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_service_label_app_kubernetes_io_name"]
        regex = "opencost"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_service_port_name"]
        regex = "http"
        action = "keep"
      }
    }

    prometheus.scrape "opencost" {
      targets    = discovery.relabel.opencost.output
      job_name   = "integrations/kubernetes/opencost"
      honor_labels = true
      scrape_interval = "60s"
      clustering {
        enabled = true
      }
      forward_to = [prometheus.relabel.opencost.receiver]
    }

    prometheus.relabel "opencost" {
      max_cache_size = 100000
      rule {
        source_labels = ["__name__"]
        regex = "up|container_cpu_allocation|container_gpu_allocation|container_memory_allocation_bytes|deployment_match_labels|kubecost_cluster_info|kubecost_cluster_management_cost|kubecost_cluster_memory_working_set_bytes|kubecost_http_requests_total|kubecost_http_response_size_bytes|kubecost_http_response_time_seconds|kubecost_load_balancer_cost|kubecost_network_internet_egress_cost|kubecost_network_region_egress_cost|kubecost_network_zone_egress_cost|kubecost_node_is_spot|node_cpu_hourly_cost|node_gpu_count|node_gpu_hourly_cost|node_ram_hourly_cost|node_total_hourly_cost|opencost_build_info|pod_pvc_allocation|pv_hourly_cost|service_selector_labels|statefulSet_match_labels"
        action = "keep"
      }
      forward_to = [prometheus.relabel.metrics_service.receiver]
    }

    // Prometheus Operator PodMonitor objects
    prometheus.operator.podmonitors "pod_monitors" {
      clustering {
        enabled = true
      }
      scrape {
        default_scrape_interval = "60s"
      }
      forward_to = [prometheus.relabel.podmonitors.receiver]
    }

    prometheus.relabel "podmonitors" {
      max_cache_size = 100000
      forward_to = [prometheus.relabel.metrics_service.receiver]
    }

    // Prometheus Operator Probe objects
    prometheus.operator.probes "probes" {
      clustering {
        enabled = true
      }
      scrape {
        default_scrape_interval = "60s"
      }
      forward_to = [prometheus.relabel.probes.receiver]
    }

    prometheus.relabel "probes" {
      max_cache_size = 100000
      forward_to = [prometheus.relabel.metrics_service.receiver]
    }

    // Prometheus Operator ServiceMonitor objects
    prometheus.operator.servicemonitors "service_monitors" {
      clustering {
        enabled = true
      }
      scrape {
        default_scrape_interval = "60s"
      }
      forward_to = [prometheus.relabel.servicemonitors.receiver]
    }

    prometheus.relabel "servicemonitors" {
      max_cache_size = 100000
      forward_to = [prometheus.relabel.metrics_service.receiver]
    }

    // Metrics Service
    remote.kubernetes.secret "metrics_service" {
      name = "prometheus-k8s-monitoring"
      namespace = "monitoring"
    }

    prometheus.relabel "metrics_service" {
      max_cache_size = 100000
      rule {
        source_labels = ["cluster"]
        regex = ""
        replacement = "aks-cluster"
        target_label = "cluster"
      }
      forward_to = [prometheus.remote_write.metrics_service.receiver]
    }

    prometheus.remote_write "metrics_service" {
      endpoint {
        url = nonsensitive(remote.kubernetes.secret.metrics_service.data["host"]) + "/api/prom/push"
        headers = { "X-Scope-OrgID" = nonsensitive(remote.kubernetes.secret.metrics_service.data["tenantId"]) }

        basic_auth {
          username = nonsensitive(remote.kubernetes.secret.metrics_service.data["username"])
          password = remote.kubernetes.secret.metrics_service.data["password"]
        }

        send_native_histograms = false
      }

      wal {
        truncate_frequency = "2h"
        min_keepalive_time = "5m"
        max_keepalive_time = "8h"
      }

      external_labels = {
      }
    }

    loki.process "pod_logs" {
      stage.match {
        selector = "{tmp_container_runtime=\"containerd\"}"
        // the cri processing stage extracts the following k/v pairs: log, stream, time, flags
        stage.cri {}

        // Set the extract flags and stream values as labels
        stage.labels {
          values = {
            flags  = "",
            stream  = "",
          }
        }
      }

      stage.match {
        selector = "{tmp_container_runtime=\"cri-o\"}"
        // the cri processing stage extracts the following k/v pairs: log, stream, time, flags
        stage.cri {}

        // Set the extract flags and stream values as labels
        stage.labels {
          values = {
            flags  = "",
            stream  = "",
          }
        }
      }

      // if the label tmp_container_runtime from above is docker parse using docker
      stage.match {
        selector = "{tmp_container_runtime=\"docker\"}"
        // the docker processing stage extracts the following k/v pairs: log, stream, time
        stage.docker {}

        // Set the extract stream value as a label
        stage.labels {
          values = {
            stream  = "",
          }
        }
      }

      // Drop the filename label, since it's not really useful in the context of Kubernetes, where we already have
      // cluster, namespace, pod, and container labels.
      // Also drop the temporary container runtime label as it is no longer needed.
      stage.label_drop {
        values = ["filename", "tmp_container_runtime"]
      }
      forward_to = [loki.process.logs_service.receiver]
    }

    // Logs Service
    remote.kubernetes.secret "logs_service" {
      name = "loki-k8s-monitoring"
      namespace = "monitoring"
    }

    loki.process "logs_service" {
      stage.static_labels {
          values = {
            cluster = "aks-cluster",
          }
      }
      forward_to = [loki.write.logs_service.receiver]
    }

    // Loki
    loki.write "logs_service" {
      endpoint {
        url = nonsensitive(remote.kubernetes.secret.logs_service.data["host"]) + "/loki/api/v1/push"
        tenant_id = nonsensitive(remote.kubernetes.secret.logs_service.data["tenantId"])

        basic_auth {
          username = nonsensitive(remote.kubernetes.secret.logs_service.data["username"])
          password = remote.kubernetes.secret.logs_service.data["password"]
        }
      }
    }


    // Tempo
    remote.kubernetes.secret "traces_service" {
      name = "tempo-k8s-monitoring"
      namespace = "monitoring"
    }

    otelcol.auth.basic "traces_service" {
      username = nonsensitive(remote.kubernetes.secret.traces_service.data["username"])
      password = remote.kubernetes.secret.traces_service.data["password"]
    }

    otelcol.exporter.otlp "traces_service" {
      client {
        endpoint = nonsensitive(remote.kubernetes.secret.traces_service.data["host"])

        auth = otelcol.auth.basic.traces_service.handler
        headers = { "X-Scope-OrgID" = nonsensitive(remote.kubernetes.secret.traces_service.data["tenantId"]) }
      }
    }

    logging {
      level  = "info"
      format = "logfmt"
    }
kind: ConfigMap
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"v1","data":{"config.alloy":"discovery.kubernetes \"nodes\" {\n  role = \"node\"\n}\n\ndiscovery.kubernetes \"services\" {\n  role = \"service\"\n}\n\ndiscovery.kubernetes \"endpoints\" {\n  role = \"endpoints\"\n}\n\ndiscovery.kubernetes \"pods\" {\n  role = \"pod\"\n}\n\n// OTLP Receivers\notelcol.receiver.otlp \"receiver\" {\n  debug_metrics {\n    disable_high_cardinality_metrics = true\n  }\n\n  grpc {\n    endpoint = \"0.0.0.0:4317\"\n  }\n\n  http {\n    endpoint = \"0.0.0.0:4318\"\n  }\n  output {\n    metrics = [otelcol.processor.resourcedetection.default.input]\n    logs = [otelcol.processor.resourcedetection.default.input]\n    traces = [\n      otelcol.processor.resourcedetection.default.input,\n      otelcol.connector.spanmetrics.default.input,\n      otelcol.connector.servicegraph.default.input,\n      ]\n  }\n}\n\n\n// Zipkin Receiver\notelcol.receiver.zipkin \"receiver\" {\n  debug_metrics {\n    disable_high_cardinality_metrics = true\n  }\n\nendpoint = \"0.0.0.0:9411\"\n  output {\n    traces = [otelcol.processor.k8sattributes.default.input]\n  }\n}\n\n\n// Processors\notelcol.processor.transform \"add_metric_datapoint_attributes\" {\n  // Grafana Cloud Kubernetes monitoring expects Loki labels `cluster`, `pod`, and `namespace`\n  error_mode = \"ignore\"\n  metric_statements {\n    context = \"datapoint\"\n    statements = [\n      \"set(attributes[\\\"deployment.environment\\\"], resource.attributes[\\\"deployment.environment\\\"])\",\n      \"set(attributes[\\\"service.version\\\"], resource.attributes[\\\"service.version\\\"])\",\n    ]\n  }\n  output {\n    metrics = [otelcol.processor.k8sattributes.default.input]\n  }\n}\n\notelcol.processor.resourcedetection \"default\" {\n  detectors = [\"env\", \"system\"]\n\n  system {\n    hostname_sources = [\"os\"]\n  }\n\n  output {\n    metrics = [otelcol.processor.transform.add_metric_datapoint_attributes.input]\n    logs    = [otelcol.processor.k8sattributes.default.input]\n    traces  = [otelcol.processor.k8sattributes.default.input]\n  }\n}\n\notelcol.processor.k8sattributes \"default\" {\n  extract {\n    metadata = [\n      \"k8s.namespace.name\",\n      \"k8s.pod.name\",\n      \"k8s.deployment.name\",\n      \"k8s.statefulset.name\",\n      \"k8s.daemonset.name\",\n      \"k8s.cronjob.name\",\n      \"k8s.job.name\",\n      \"k8s.node.name\",\n      \"k8s.pod.uid\",\n      \"k8s.pod.start_time\",\n    ]\n  }\n  pod_association {\n    source {\n      from = \"connection\"\n    }\n  }\n\n  output {\n    metrics = [otelcol.processor.transform.default.input]\n    logs    = [otelcol.processor.transform.default.input]\n    traces  = [\n      otelcol.processor.transform.default.input,\n      otelcol.connector.host_info.default.input,\n    ]\n  }\n}\notelcol.connector.host_info \"default\" {\n  host_identifiers = [ \"k8s.node.name\" ]\n  output {\n    metrics = [otelcol.processor.batch.host_info_batch.input]\n  }\n}\n\notelcol.processor.batch \"host_info_batch\" {\n  output {\n    metrics = [otelcol.exporter.prometheus.host_info_metrics.input]\n  }\n}\n\notelcol.exporter.prometheus \"host_info_metrics\" {\n  add_metric_suffixes = false\n  forward_to = [prometheus.remote_write.metrics_service.receiver]\n}\n\n// The Spanmetrics Connector will generate RED metrics based on the incoming trace span data.\notelcol.connector.spanmetrics \"default\" {\n    // The namespace explicit adds a prefix to all the generated span metrics names.\n    // In this case, we'll ensure they match as closely as possible those generated by Tempo.\n\n    // Each extra dimension (metrics label) to be added to the generated metrics from matching span attributes. These\n    // need to be defined with a name and optionally a default value (in the following cases, we do not want a default\n    // value if the span attribute is not present).\n    dimension {\n        name = \"http.method\"\n    }\n    dimension {\n        name = \"http.status_code\"\n    }\n    metrics_flush_interval = \"5s\"\n\n    // A histogram block must be present, either explicitly defining bucket values or via an exponential block.\n    // We do the latter here.\n    histogram {\n        explicit {\n          buckets=  [\"10ms\", \"50ms\", \"100ms\", \"200ms\", \"400ms\", \"1s\"]\n        }\n    }\n\n    // The exemplar block is added to ensure we generate exemplars for traces on relevant metric values.\n    exemplars {\n        enabled = true\n    }\n\n    // Generated metrics data is in OTLP format. We send this data to the OpenTelemetry Prometheus exporter to ensure\n    // it gets transformed into Prometheus format data.\n    output {\n        // metrics = [otelcol.exporter.prometheus.metrics_converter.input]\n        metrics = [otelcol.processor.transform.add_metric_datapoint_attributes.input]\n    }\n}\n\notelcol.connector.servicegraph \"default\" {\n   dimensions = [\"http.method\"]\n   output {\n      metrics = [otelcol.processor.transform.add_metric_datapoint_attributes.input]\n   }\n}\n\notelcol.processor.transform \"default\" {\n  // Grafana Cloud Kubernetes monitoring expects Loki labels `cluster`, `pod`, and `namespace`\n  error_mode = \"ignore\"\n  metric_statements {\n    context = \"resource\"\n    statements = [\n      \"set(attributes[\\\"k8s.cluster.name\\\"], \\\"aks-cluster\\\") where attributes[\\\"k8s.cluster.name\\\"] == nil\",\n    ]\n  }\n  log_statements {\n    context = \"resource\"\n    statements = [\n      \"set(attributes[\\\"pod\\\"], attributes[\\\"k8s.pod.name\\\"])\",\n      \"set(attributes[\\\"namespace\\\"], attributes[\\\"k8s.namespace.name\\\"])\",\n      \"set(attributes[\\\"loki.resource.labels\\\"], \\\"pod, namespace, cluster, job\\\")\",\n      \"set(attributes[\\\"k8s.cluster.name\\\"], \\\"aks-cluster\\\") where attributes[\\\"k8s.cluster.name\\\"] == nil\",\n    ]\n  }\n  trace_statements {\n    context = \"resource\"\n    statements = [\n      \"set(attributes[\\\"k8s.cluster.name\\\"], \\\"aks-cluster\\\") where attributes[\\\"k8s.cluster.name\\\"] == nil\",\n    ]\n  }\n  output {\n    metrics = [otelcol.processor.filter.default.input]\n    logs = [otelcol.processor.filter.default.input]\n    traces = [otelcol.processor.filter.default.input]\n  }\n}\n\notelcol.processor.filter \"default\" {\n  error_mode = \"ignore\"\n\n  output {\n    metrics = [otelcol.processor.batch.batch_processor.input]\n    logs = [otelcol.processor.batch.batch_processor.input]\n    traces = [otelcol.processor.batch.batch_processor.input]\n  }\n}\n\notelcol.processor.batch \"batch_processor\" {\n  send_batch_size = 16384\n  send_batch_max_size = 0\n  timeout = \"2s\"\n  output {\n    metrics = [otelcol.exporter.prometheus.metrics_converter.input]\n    logs = [otelcol.exporter.loki.logs_converter.input]\n    traces = [otelcol.exporter.otlp.traces_service.input]\n  }\n}\notelcol.exporter.prometheus \"metrics_converter\" {\n  forward_to = [prometheus.relabel.metrics_service.receiver]\n}\notelcol.exporter.loki \"logs_converter\" {\n  forward_to = [loki.process.pod_logs.receiver]\n}\n// Annotation Autodiscovery\ndiscovery.relabel \"annotation_autodiscovery_pods\" {\n  targets = discovery.kubernetes.pods.targets\n  rule {\n    source_labels = [\"__meta_kubernetes_pod_annotation_k8s_grafana_com_scrape\"]\n    regex = \"true\"\n    action = \"keep\"\n  }\n  rule {\n    source_labels = [\"__meta_kubernetes_pod_annotation_k8s_grafana_com_job\"]\n    action = \"replace\"\n    target_label = \"job\"\n  }\n  rule {\n    source_labels = [\"__meta_kubernetes_pod_annotation_k8s_grafana_com_instance\"]\n    action = \"replace\"\n    target_label = \"instance\"\n  }\n  rule {\n    source_labels = [\"__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_path\"]\n    action = \"replace\"\n    target_label = \"__metrics_path__\"\n  }\n\n  // Choose the pod port\n  // The discovery generates a target for each declared container port of the pod.\n  // If the metricsPortName annotation has value, keep only the target where the port name matches the one of the annotation.\n  rule {\n    source_labels = [\"__meta_kubernetes_pod_container_port_name\"]\n    target_label = \"__tmp_port\"\n  }\n  rule {\n    source_labels = [\"__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_portName\"]\n    regex = \"(.+)\"\n    target_label = \"__tmp_port\"\n  }\n  rule {\n    source_labels = [\"__meta_kubernetes_pod_container_port_name\"]\n    action = \"keepequal\"\n    target_label = \"__tmp_port\"\n  }\n\n  // If the metrics port number annotation has a value, override the target address to use it, regardless whether it is\n  // one of the declared ports on that Pod.\n  rule {\n    source_labels = [\"__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_portNumber\", \"__meta_kubernetes_pod_ip\"]\n    regex = \"(\\\\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})\"\n    replacement = \"[$2]:$1\" // IPv6\n    target_label = \"__address__\"\n  }\n  rule {\n    source_labels = [\"__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_portNumber\", \"__meta_kubernetes_pod_ip\"]\n    regex = \"(\\\\d+);((([0-9]+?)(\\\\.|$)){4})\" // IPv4, takes priority over IPv6 when both exists\n    replacement = \"$2:$1\"\n    target_label = \"__address__\"\n  }\n\n  rule {\n    source_labels = [\"__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_scheme\"]\n    action = \"replace\"\n    target_label = \"__scheme__\"\n  }\n}\n\ndiscovery.relabel \"annotation_autodiscovery_services\" {\n  targets = discovery.kubernetes.services.targets\n  rule {\n    source_labels = [\"__meta_kubernetes_service_annotation_k8s_grafana_com_scrape\"]\n    regex = \"true\"\n    action = \"keep\"\n  }\n  rule {\n    source_labels = [\"__meta_kubernetes_service_annotation_k8s_grafana_com_job\"]\n    action = \"replace\"\n    target_label = \"job\"\n  }\n  rule {\n    source_labels = [\"__meta_kubernetes_service_annotation_k8s_grafana_com_instance\"]\n    action = \"replace\"\n    target_label = \"instance\"\n  }\n  rule {\n    source_labels = [\"__meta_kubernetes_service_annotation_k8s_grafana_com_metrics_path\"]\n    action = \"replace\"\n    target_label = \"__metrics_path__\"\n  }\n\n  // Choose the service port\n  rule {\n    source_labels = [\"__meta_kubernetes_service_port_name\"]\n    target_label = \"__tmp_port\"\n  }\n  rule {\n    source_labels = [\"__meta_kubernetes_service_annotation_k8s_grafana_com_metrics_portName\"]\n    regex = \"(.+)\"\n    target_label = \"__tmp_port\"\n  }\n  rule {\n    source_labels = [\"__meta_kubernetes_service_port_name\"]\n    action = \"keepequal\"\n    target_label = \"__tmp_port\"\n  }\n\n  rule {\n    source_labels = [\"__meta_kubernetes_service_port_number\"]\n    target_label = \"__tmp_port\"\n  }\n  rule {\n    source_labels = [\"__meta_kubernetes_service_annotation_k8s_grafana_com_metrics_portNumber\"]\n    regex = \"(.+)\"\n    target_label = \"__tmp_port\"\n  }\n  rule {\n    source_labels = [\"__meta_kubernetes_service_port_number\"]\n    action = \"keepequal\"\n    target_label = \"__tmp_port\"\n  }\n\n  rule {\n    source_labels = [\"__meta_kubernetes_service_annotation_k8s_grafana_com_metrics_scheme\"]\n    action = \"replace\"\n    target_label = \"__scheme__\"\n  }\n}\n\ndiscovery.relabel \"annotation_autodiscovery_http\" {\n  targets = concat(discovery.relabel.annotation_autodiscovery_pods.output, discovery.relabel.annotation_autodiscovery_services.output)\n  rule {\n    source_labels = [\"__scheme__\"]\n    regex = \"https\"\n    action = \"drop\"\n  }\n}\n\ndiscovery.relabel \"annotation_autodiscovery_https\" {\n  targets = concat(discovery.relabel.annotation_autodiscovery_pods.output, discovery.relabel.annotation_autodiscovery_services.output)\n  rule {\n    source_labels = [\"__scheme__\"]\n    regex = \"https\"\n    action = \"keep\"\n  }\n}\n\nprometheus.scrape \"annotation_autodiscovery_http\" {\n  targets = discovery.relabel.annotation_autodiscovery_http.output\n  honor_labels = true\n  clustering {\n    enabled = true\n  }\n  forward_to = [prometheus.relabel.annotation_autodiscovery.receiver]\n}\n\nprometheus.scrape \"annotation_autodiscovery_https\" {\n  targets = discovery.relabel.annotation_autodiscovery_https.output\n  honor_labels = true\n  bearer_token_file = \"/var/run/secrets/kubernetes.io/serviceaccount/token\"\n  tls_config {\n    insecure_skip_verify = true\n  }\n  clustering {\n    enabled = true\n  }\n  forward_to = [prometheus.relabel.annotation_autodiscovery.receiver]\n}\n\nprometheus.relabel \"annotation_autodiscovery\" {\n  max_cache_size = 100000\n  forward_to = [prometheus.relabel.metrics_service.receiver]\n}\n\n// Grafana Alloy\ndiscovery.relabel \"alloy\" {\n  targets = discovery.kubernetes.pods.targets\n  rule {\n    source_labels = [\"__meta_kubernetes_pod_label_app_kubernetes_io_instance\"]\n    regex = \"grafana-k8s-monitoring\"\n    action = \"keep\"\n  }\n  rule {\n    source_labels = [\"__meta_kubernetes_pod_label_app_kubernetes_io_name\"]\n    regex = \"alloy.*\"\n    action = \"keep\"\n  }\n  rule {\n    source_labels = [\"__meta_kubernetes_pod_container_port_name\"]\n    regex = \"http-metrics\"\n    action = \"keep\"\n  }\n  rule {\n    source_labels = [\"__meta_kubernetes_namespace\"]\n    target_label  = \"namespace\"\n  }\n  rule {\n    source_labels = [\"__meta_kubernetes_pod_name\"]\n    target_label  = \"pod\"\n  }\n  rule {\n    source_labels = [\"__meta_kubernetes_pod_container_name\"]\n    target_label  = \"container\"\n  }\n}\n\nprometheus.scrape \"alloy\" {\n  job_name = \"integrations/alloy\"\n  targets = discovery.relabel.alloy.output\n  scrape_interval = \"60s\"\n  forward_to = [prometheus.relabel.alloy.receiver]\n  clustering {\n    enabled = true\n  }\n}\n\nprometheus.relabel \"alloy\" {\n  max_cache_size = 100000\n  rule {\n    source_labels = [\"__name__\"]\n    regex = \"up|alloy_build_info\"\n    action = \"keep\"\n  }\n  forward_to = [prometheus.relabel.metrics_service.receiver]\n}\n\n// Kubernetes Monitoring Telemetry\nprometheus.exporter.unix \"kubernetes_monitoring_telemetry\" {\n  set_collectors = [\"textfile\"]\n  textfile {\n    directory = \"/etc/kubernetes-monitoring-telemetry\"\n  }\n}\n\nprometheus.scrape \"kubernetes_monitoring_telemetry\" {\n  job_name   = \"integrations/kubernetes/kubernetes_monitoring_telemetry\"\n  targets    = prometheus.exporter.unix.kubernetes_monitoring_telemetry.targets\n  scrape_interval = \"60s\"\n  clustering {\n    enabled = true\n  }\n  forward_to = [prometheus.relabel.kubernetes_monitoring_telemetry.receiver]\n}\n\nprometheus.relabel \"kubernetes_monitoring_telemetry\" {\n  max_cache_size = 100000\n  rule {\n    target_label = \"job\"\n    action = \"replace\"\n    replacement = \"integrations/kubernetes/kubernetes_monitoring_telemetry\"\n  }\n  rule {\n    target_label = \"instance\"\n    action = \"replace\"\n    replacement = \"grafana-k8s-monitoring\"\n  }\n  rule {\n    source_labels = [\"__name__\"]\n    regex = \"up|grafana_kubernetes_monitoring_.*\"\n    action = \"keep\"\n  }\n  forward_to = [prometheus.relabel.metrics_service.receiver]\n}\n\n// Kubelet\ndiscovery.relabel \"kubelet\" {\n  targets = discovery.kubernetes.nodes.targets\n}\n\nprometheus.scrape \"kubelet\" {\n  job_name   = \"integrations/kubernetes/kubelet\"\n  targets  = discovery.relabel.kubelet.output\n  scheme   = \"https\"\n  scrape_interval = \"60s\"\n  bearer_token_file = \"/var/run/secrets/kubernetes.io/serviceaccount/token\"\n  tls_config {\n    insecure_skip_verify = true\n  }\n  clustering {\n    enabled = true\n  }\n  forward_to = [prometheus.relabel.kubelet.receiver]\n}\n\nprometheus.relabel \"kubelet\" {\n  max_cache_size = 100000\n  rule {\n    source_labels = [\"__name__\"]\n    regex = \"up|container_cpu_usage_seconds_total|kubelet_certificate_manager_client_expiration_renew_errors|kubelet_certificate_manager_client_ttl_seconds|kubelet_certificate_manager_server_ttl_seconds|kubelet_cgroup_manager_duration_seconds_bucket|kubelet_cgroup_manager_duration_seconds_count|kubelet_node_config_error|kubelet_node_name|kubelet_pleg_relist_duration_seconds_bucket|kubelet_pleg_relist_duration_seconds_count|kubelet_pleg_relist_interval_seconds_bucket|kubelet_pod_start_duration_seconds_bucket|kubelet_pod_start_duration_seconds_count|kubelet_pod_worker_duration_seconds_bucket|kubelet_pod_worker_duration_seconds_count|kubelet_running_container_count|kubelet_running_containers|kubelet_running_pod_count|kubelet_running_pods|kubelet_runtime_operations_errors_total|kubelet_runtime_operations_total|kubelet_server_expiration_renew_errors|kubelet_volume_stats_available_bytes|kubelet_volume_stats_capacity_bytes|kubelet_volume_stats_inodes|kubelet_volume_stats_inodes_used|kubernetes_build_info|namespace_workload_pod|rest_client_requests_total|storage_operation_duration_seconds_count|storage_operation_errors_total|volume_manager_total_volumes\"\n    action = \"keep\"\n  }\n  forward_to = [prometheus.relabel.metrics_service.receiver]\n}\n\n// cAdvisor\ndiscovery.relabel \"cadvisor\" {\n  targets = discovery.kubernetes.nodes.targets\n  rule {\n    replacement   = \"/metrics/cadvisor\"\n    target_label  = \"__metrics_path__\"\n  }\n}\n\nprometheus.scrape \"cadvisor\" {\n  job_name   = \"integrations/kubernetes/cadvisor\"\n  targets    = discovery.relabel.cadvisor.output\n  scheme     = \"https\"\n  scrape_interval = \"60s\"\n  bearer_token_file = \"/var/run/secrets/kubernetes.io/serviceaccount/token\"\n  tls_config {\n    insecure_skip_verify = true\n  }\n  clustering {\n    enabled = true\n  }\n  forward_to = [prometheus.relabel.cadvisor.receiver]\n}\n\nprometheus.relabel \"cadvisor\" {\n  max_cache_size = 100000\n  rule {\n    source_labels = [\"__name__\"]\n    regex = \"up|container_cpu_cfs_periods_total|container_cpu_cfs_throttled_periods_total|container_cpu_usage_seconds_total|container_fs_reads_bytes_total|container_fs_reads_total|container_fs_writes_bytes_total|container_fs_writes_total|container_memory_cache|container_memory_rss|container_memory_swap|container_memory_working_set_bytes|container_network_receive_bytes_total|container_network_receive_packets_dropped_total|container_network_receive_packets_total|container_network_transmit_bytes_total|container_network_transmit_packets_dropped_total|container_network_transmit_packets_total|machine_memory_bytes\"\n    action = \"keep\"\n  }\n  // Drop empty container labels, addressing https://github.com/google/cadvisor/issues/2688\n  rule {\n    source_labels = [\"__name__\",\"container\"]\n    separator = \"@\"\n    regex = \"(container_cpu_.*|container_fs_.*|container_memory_.*)@\"\n    action = \"drop\"\n  }\n  // Drop empty image labels, addressing https://github.com/google/cadvisor/issues/2688\n  rule {\n    source_labels = [\"__name__\",\"image\"]\n    separator = \"@\"\n    regex = \"(container_cpu_.*|container_fs_.*|container_memory_.*|container_network_.*)@\"\n    action = \"drop\"\n  }\n  // Normalizing unimportant labels (not deleting to continue satisfying \u003clabel\u003e!=\"\" checks)\n  rule {\n    source_labels = [\"__name__\", \"boot_id\"]\n    separator = \"@\"\n    regex = \"machine_memory_bytes@.*\"\n    target_label = \"boot_id\"\n    replacement = \"NA\"\n  }\n  rule {\n    source_labels = [\"__name__\", \"system_uuid\"]\n    separator = \"@\"\n    regex = \"machine_memory_bytes@.*\"\n    target_label = \"system_uuid\"\n    replacement = \"NA\"\n  }\n  // Filter out non-physical devices/interfaces\n  rule {\n    source_labels = [\"__name__\", \"device\"]\n    separator = \"@\"\n    regex = \"container_fs_.*@(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dasd.+)\"\n    target_label = \"__keepme\"\n    replacement = \"1\"\n  }\n  rule {\n    source_labels = [\"__name__\", \"__keepme\"]\n    separator = \"@\"\n    regex = \"container_fs_.*@\"\n    action = \"drop\"\n  }\n  rule {\n    source_labels = [\"__name__\"]\n    regex = \"container_fs_.*\"\n    target_label = \"__keepme\"\n    replacement = \"\"\n  }\n  rule {\n    source_labels = [\"__name__\", \"interface\"]\n    separator = \"@\"\n    regex = \"container_network_.*@(en[ospx][0-9].*|wlan[0-9].*|eth[0-9].*)\"\n    target_label = \"__keepme\"\n    replacement = \"1\"\n  }\n  rule {\n    source_labels = [\"__name__\", \"__keepme\"]\n    separator = \"@\"\n    regex = \"container_network_.*@\"\n    action = \"drop\"\n  }\n  rule {\n    source_labels = [\"__name__\"]\n    regex = \"container_network_.*\"\n    target_label = \"__keepme\"\n    replacement = \"\"\n  }\n  forward_to = [prometheus.relabel.metrics_service.receiver]\n}\n\n// Kube State Metrics\ndiscovery.relabel \"kube_state_metrics\" {\n  targets = discovery.kubernetes.services.targets\n  rule {\n    source_labels = [\"__meta_kubernetes_service_label_app_kubernetes_io_instance\"]\n    regex = \"grafana-k8s-monitoring\"\n    action = \"keep\"\n  }\n  rule {\n    source_labels = [\"__meta_kubernetes_service_label_app_kubernetes_io_name\"]\n    regex = \"kube-state-metrics\"\n    action = \"keep\"\n  }\n  rule {\n    source_labels = [\"__meta_kubernetes_service_port_name\"]\n    regex = \"http\"\n    action = \"keep\"\n  }\n}\n\nprometheus.scrape \"kube_state_metrics\" {\n  job_name   = \"integrations/kubernetes/kube-state-metrics\"\n  targets    = discovery.relabel.kube_state_metrics.output\n  scrape_interval = \"60s\"\n  clustering {\n    enabled = true\n  }\n  forward_to = [prometheus.relabel.kube_state_metrics.receiver]\n}\n\nprometheus.relabel \"kube_state_metrics\" {\n  max_cache_size = 100000\n  rule {\n    source_labels = [\"__name__\"]\n    regex = \"up|kube_daemonset.*|kube_deployment_metadata_generation|kube_deployment_spec_replicas|kube_deployment_status_observed_generation|kube_deployment_status_replicas_available|kube_deployment_status_replicas_updated|kube_horizontalpodautoscaler_spec_max_replicas|kube_horizontalpodautoscaler_spec_min_replicas|kube_horizontalpodautoscaler_status_current_replicas|kube_horizontalpodautoscaler_status_desired_replicas|kube_job.*|kube_namespace_status_phase|kube_node.*|kube_persistentvolumeclaim_resource_requests_storage_bytes|kube_pod_container_info|kube_pod_container_resource_limits|kube_pod_container_resource_requests|kube_pod_container_status_last_terminated_reason|kube_pod_container_status_restarts_total|kube_pod_container_status_waiting_reason|kube_pod_info|kube_pod_owner|kube_pod_start_time|kube_pod_status_phase|kube_pod_status_reason|kube_replicaset.*|kube_resourcequota|kube_statefulset.*\"\n    action = \"keep\"\n  }\n  forward_to = [prometheus.relabel.metrics_service.receiver]\n}\n\n// Node Exporter\ndiscovery.relabel \"node_exporter\" {\n  targets = discovery.kubernetes.pods.targets\n  rule {\n    source_labels = [\"__meta_kubernetes_pod_label_app_kubernetes_io_instance\"]\n    regex = \"grafana-k8s-monitoring\"\n    action = \"keep\"\n  }\n  rule {\n    source_labels = [\"__meta_kubernetes_pod_label_app_kubernetes_io_name\"]\n    regex = \"prometheus-node-exporter.*\"\n    action = \"keep\"\n  }\n  rule {\n    source_labels = [\"__meta_kubernetes_pod_node_name\"]\n    action = \"replace\"\n    target_label = \"instance\"\n  }\n}\n\nprometheus.scrape \"node_exporter\" {\n  job_name   = \"integrations/node_exporter\"\n  targets  = discovery.relabel.node_exporter.output\n  scrape_interval = \"60s\"\n  clustering {\n    enabled = true\n  }\n  forward_to = [prometheus.relabel.node_exporter.receiver]\n}\n\nprometheus.relabel \"node_exporter\" {\n  max_cache_size = 100000\n  rule {\n    source_labels = [\"__name__\"]\n    regex = \"up|node_cpu.*|node_exporter_build_info|node_filesystem.*|node_memory.*|process_cpu_seconds_total|process_resident_memory_bytes\"\n    action = \"keep\"\n  }\n  // Drop metrics for certain file systems\n  rule {\n    source_labels = [\"__name__\", \"fstype\"]\n    separator = \"@\"\n    regex = \"node_filesystem.*@(tempfs)\"\n    action = \"drop\"\n  }\n  forward_to = [prometheus.relabel.metrics_service.receiver]\n}\n\n// OpenCost\ndiscovery.relabel \"opencost\" {\n  targets = discovery.kubernetes.services.targets\n  rule {\n    source_labels = [\"__meta_kubernetes_service_label_app_kubernetes_io_instance\"]\n    regex = \"grafana-k8s-monitoring\"\n    action = \"keep\"\n  }\n  rule {\n    source_labels = [\"__meta_kubernetes_service_label_app_kubernetes_io_name\"]\n    regex = \"opencost\"\n    action = \"keep\"\n  }\n  rule {\n    source_labels = [\"__meta_kubernetes_service_port_name\"]\n    regex = \"http\"\n    action = \"keep\"\n  }\n}\n\nprometheus.scrape \"opencost\" {\n  targets    = discovery.relabel.opencost.output\n  job_name   = \"integrations/kubernetes/opencost\"\n  honor_labels = true\n  scrape_interval = \"60s\"\n  clustering {\n    enabled = true\n  }\n  forward_to = [prometheus.relabel.opencost.receiver]\n}\n\nprometheus.relabel \"opencost\" {\n  max_cache_size = 100000\n  rule {\n    source_labels = [\"__name__\"]\n    regex = \"up|container_cpu_allocation|container_gpu_allocation|container_memory_allocation_bytes|deployment_match_labels|kubecost_cluster_info|kubecost_cluster_management_cost|kubecost_cluster_memory_working_set_bytes|kubecost_http_requests_total|kubecost_http_response_size_bytes|kubecost_http_response_time_seconds|kubecost_load_balancer_cost|kubecost_network_internet_egress_cost|kubecost_network_region_egress_cost|kubecost_network_zone_egress_cost|kubecost_node_is_spot|node_cpu_hourly_cost|node_gpu_count|node_gpu_hourly_cost|node_ram_hourly_cost|node_total_hourly_cost|opencost_build_info|pod_pvc_allocation|pv_hourly_cost|service_selector_labels|statefulSet_match_labels\"\n    action = \"keep\"\n  }\n  forward_to = [prometheus.relabel.metrics_service.receiver]\n}\n\n// Prometheus Operator PodMonitor objects\nprometheus.operator.podmonitors \"pod_monitors\" {\n  clustering {\n    enabled = true\n  }\n  scrape {\n    default_scrape_interval = \"60s\"\n  }\n  forward_to = [prometheus.relabel.podmonitors.receiver]\n}\n\nprometheus.relabel \"podmonitors\" {\n  max_cache_size = 100000\n  forward_to = [prometheus.relabel.metrics_service.receiver]\n}\n\n// Prometheus Operator Probe objects\nprometheus.operator.probes \"probes\" {\n  clustering {\n    enabled = true\n  }\n  scrape {\n    default_scrape_interval = \"60s\"\n  }\n  forward_to = [prometheus.relabel.probes.receiver]\n}\n\nprometheus.relabel \"probes\" {\n  max_cache_size = 100000\n  forward_to = [prometheus.relabel.metrics_service.receiver]\n}\n\n// Prometheus Operator ServiceMonitor objects\nprometheus.operator.servicemonitors \"service_monitors\" {\n  clustering {\n    enabled = true\n  }\n  scrape {\n    default_scrape_interval = \"60s\"\n  }\n  forward_to = [prometheus.relabel.servicemonitors.receiver]\n}\n\nprometheus.relabel \"servicemonitors\" {\n  max_cache_size = 100000\n  forward_to = [prometheus.relabel.metrics_service.receiver]\n}\n\n// Metrics Service\nremote.kubernetes.secret \"metrics_service\" {\n  name = \"prometheus-k8s-monitoring\"\n  namespace = \"monitoring\"\n}\n\nprometheus.relabel \"metrics_service\" {\n  max_cache_size = 100000\n  rule {\n    source_labels = [\"cluster\"]\n    regex = \"\"\n    replacement = \"aks-cluster\"\n    target_label = \"cluster\"\n  }\n  forward_to = [prometheus.remote_write.metrics_service.receiver]\n}\n\nprometheus.remote_write \"metrics_service\" {\n  endpoint {\n    url = nonsensitive(remote.kubernetes.secret.metrics_service.data[\"host\"]) + \"/api/prom/push\"\n    headers = { \"X-Scope-OrgID\" = nonsensitive(remote.kubernetes.secret.metrics_service.data[\"tenantId\"]) }\n\n    basic_auth {\n      username = nonsensitive(remote.kubernetes.secret.metrics_service.data[\"username\"])\n      password = remote.kubernetes.secret.metrics_service.data[\"password\"]\n    }\n\n    send_native_histograms = false\n  }\n\n  wal {\n    truncate_frequency = \"2h\"\n    min_keepalive_time = \"5m\"\n    max_keepalive_time = \"8h\"\n  }\n\n  external_labels = {\n  }\n}\n\nloki.process \"pod_logs\" {\n  stage.match {\n    selector = \"{tmp_container_runtime=\\\"containerd\\\"}\"\n    // the cri processing stage extracts the following k/v pairs: log, stream, time, flags\n    stage.cri {}\n\n    // Set the extract flags and stream values as labels\n    stage.labels {\n      values = {\n        flags  = \"\",\n        stream  = \"\",\n      }\n    }\n  }\n\n  stage.match {\n    selector = \"{tmp_container_runtime=\\\"cri-o\\\"}\"\n    // the cri processing stage extracts the following k/v pairs: log, stream, time, flags\n    stage.cri {}\n\n    // Set the extract flags and stream values as labels\n    stage.labels {\n      values = {\n        flags  = \"\",\n        stream  = \"\",\n      }\n    }\n  }\n\n  // if the label tmp_container_runtime from above is docker parse using docker\n  stage.match {\n    selector = \"{tmp_container_runtime=\\\"docker\\\"}\"\n    // the docker processing stage extracts the following k/v pairs: log, stream, time\n    stage.docker {}\n\n    // Set the extract stream value as a label\n    stage.labels {\n      values = {\n        stream  = \"\",\n      }\n    }\n  }\n\n  // Drop the filename label, since it's not really useful in the context of Kubernetes, where we already have\n  // cluster, namespace, pod, and container labels.\n  // Also drop the temporary container runtime label as it is no longer needed.\n  stage.label_drop {\n    values = [\"filename\", \"tmp_container_runtime\"]\n  }\n  forward_to = [loki.process.logs_service.receiver]\n}\n\n// Logs Service\nremote.kubernetes.secret \"logs_service\" {\n  name = \"loki-k8s-monitoring\"\n  namespace = \"monitoring\"\n}\n\nloki.process \"logs_service\" {\n  stage.static_labels {\n      values = {\n        cluster = \"aks-cluster\",\n      }\n  }\n  forward_to = [loki.write.logs_service.receiver]\n}\n\n// Loki\nloki.write \"logs_service\" {\n  endpoint {\n    url = nonsensitive(remote.kubernetes.secret.logs_service.data[\"host\"]) + \"/loki/api/v1/push\"\n    tenant_id = nonsensitive(remote.kubernetes.secret.logs_service.data[\"tenantId\"])\n\n    basic_auth {\n      username = nonsensitive(remote.kubernetes.secret.logs_service.data[\"username\"])\n      password = remote.kubernetes.secret.logs_service.data[\"password\"]\n    }\n  }\n}\n\n\n// Tempo\nremote.kubernetes.secret \"traces_service\" {\n  name = \"tempo-k8s-monitoring\"\n  namespace = \"monitoring\"\n}\n\notelcol.auth.basic \"traces_service\" {\n  username = nonsensitive(remote.kubernetes.secret.traces_service.data[\"username\"])\n  password = remote.kubernetes.secret.traces_service.data[\"password\"]\n}\n\notelcol.exporter.otlp \"traces_service\" {\n  client {\n    endpoint = nonsensitive(remote.kubernetes.secret.traces_service.data[\"host\"])\n\n    auth = otelcol.auth.basic.traces_service.handler\n    headers = { \"X-Scope-OrgID\" = nonsensitive(remote.kubernetes.secret.traces_service.data[\"tenantId\"]) }\n  }\n}\n\nlogging {\n  level  = \"info\"\n  format = \"logfmt\"\n}"},"kind":"ConfigMap","metadata":{"annotations":{"meta.helm.sh/release-name":"grafana-k8s-monitoring","meta.helm.sh/release-namespace":"monitoring"},"creationTimestamp":"2024-05-11T10:31:48Z","labels":{"app.kubernetes.io/managed-by":"Helm"},"name":"grafana-k8s-monitoring-alloy","namespace":"monitoring","resourceVersion":"83451922","uid":"99166cde-f6e6-4f7c-b258-c1c5f2d2fccb"}}
    meta.helm.sh/release-name: grafana-k8s-monitoring
    meta.helm.sh/release-namespace: monitoring
  creationTimestamp: "2024-05-11T10:31:48Z"
  labels:
    app.kubernetes.io/managed-by: Helm
  name: grafana-k8s-monitoring-alloy
  namespace: monitoring
  resourceVersion: "83774711"
  uid: 99166cde-f6e6-4f7c-b258-c1c5f2d2fccb
